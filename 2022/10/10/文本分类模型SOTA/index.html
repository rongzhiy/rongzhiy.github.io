<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>文本分类模型SOTA | Rongzhiyのblog</title><meta name="keywords" content="机器学习"><meta name="author" content="阿泽"><meta name="copyright" content="阿泽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="本文将分 3 期进行连载，共介绍 20 个在文本分类任务上曾取得 SOTA 的经典模型。  第 1 期：RAE、DAN、TextRCNN、Multi-task、DeepMoji、RNN-Capsule 第 2 期：TextCNN、dcnn、XML-CNN、textCapsule、Bao et al.、AttentionXML 第 3 期：ELMo、GPT、BERT、ALBERT、X-Transfo">
<meta property="og:type" content="article">
<meta property="og:title" content="文本分类模型SOTA">
<meta property="og:url" content="https://rongzhiy.github.io/2022/10/10/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8BSOTA/index.html">
<meta property="og:site_name" content="Rongzhiyのblog">
<meta property="og:description" content="本文将分 3 期进行连载，共介绍 20 个在文本分类任务上曾取得 SOTA 的经典模型。  第 1 期：RAE、DAN、TextRCNN、Multi-task、DeepMoji、RNN-Capsule 第 2 期：TextCNN、dcnn、XML-CNN、textCapsule、Bao et al.、AttentionXML 第 3 期：ELMo、GPT、BERT、ALBERT、X-Transfo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2022-10-10T15:32:44.000Z">
<meta property="article:modified_time" content="2022-10-22T15:42:26.955Z">
<meta property="article:author" content="阿泽">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://rongzhiy.github.io/2022/10/10/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8BSOTA/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-10-22 23:42:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="/css/cat.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">204</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Rongzhiyのblog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">文本分类模型SOTA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-10T15:32:44.000Z" title="发表于 2022-10-10 23:32:44">2022-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-10-22T15:42:26.955Z" title="更新于 2022-10-22 23:42:26">2022-10-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文本分类模型SOTA"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><p>本文将分 3 期进行连载，共介绍 <strong>20 个</strong>在<strong>文本分类</strong>任务上曾取得 SOTA 的经典模型。</p>
<ul>
<li>第 1 期：RAE、DAN、TextRCNN、Multi-task、DeepMoji、RNN-Capsule</li>
<li>第 2 期：TextCNN、dcnn、XML-CNN、textCapsule、Bao et al.、AttentionXML</li>
<li>第 3 期：ELMo、GPT、BERT、ALBERT、X-Transformer、LightXML、TextGCN、TensorGCN</li>
</ul>
<p>文本分类是自然语言处理中最基本、最经典的任务，大部分自然语言处理任务都可以看作是分类任务。<br>与数字、图像不同，对文本的处理强调精细化的处理能力。传统的文本分类方法一般需要对输入模型的文本数据进行预处理，此外还需要通过人工标注的方法来获得良好的样本特征，然后使用经典的机器学习算法对其进行分类。类似的方法包括 NaiveBayes（NB）、K 近邻（KNN）、支持向量机 SVM 等。特征提取的水平对文本分类效果的影响甚至高于图像分类，而文本分类中的特征工程往往非常耗时且计算成本高。2010 年后，文本分类的方法逐渐过渡到深度学习模型。应用于文本分类的深度学习通过学习一系列的非线性变换模式将特征工程直接映射到输出，从而将特征工程集成到模型拟合过程中，一经应用就获得了巨大的成功。</p>
<p><strong>与图像分类模型不同，文本分类模型一般不会采用堆叠模块、修改深度模型结构等方式去改进，更多则是通过引入其它技术手段改进模型效果，例如引入注意力机制、预训练、图神经网络、胶囊网络等</strong>。所以在介绍经典文本分类模型时，更多的是介绍为了解决文本分类中的哪一类具体问题，针对性地引入了哪些专门的技术 trick，以及这些引入的 trick 是如何与原有的经典架构融合的。</p>
<p>此外，NLP 领域中大量工作都聚焦于前端的词、语句、文本的处理或语义理解，目的是为下游的各类任务服务。</p>
<p><strong>文本分类模型以 BERT 的出现明显呈现出两个不同的发展阶段，BERT 提出后（2019 年之后），单纯基于 RNN、CNN 改进的且效果比较突出的方法就比较少了。</strong></p>
<h1 id="一、-RNN"><a href="#一、-RNN" class="headerlink" title="一、 RNN"></a>一、 RNN</h1><p>递归神经网络（Recurrent Neural Network，RNN）被广泛用于通过递归计算捕捉长距离的依赖性。RNN 语言模型学习历史信息，考虑到适合文本分类任务的所有单词之间的位置信息。首先，每个输入词都用一个特定的向量表示，使用词嵌入技术。然后，嵌入的单词向量被逐一送入RNN 单元。RNN 单元的输出与输入向量的维度相同，并被送入下一个隐藏层。RNN 在模型的不同部分共享参数，每个输入词的权重相同。最后，输入文本的标签可以由隐藏层的最后一个输出来预测。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665385743110-8ef7501f-acce-46a7-bda5-05afece7aa76.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u6e277e9c&margin=%5Bobject%20Object%5D&originHeight=664&originWidth=1028&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u583778e5-06cd-4b25-8216-a9737ae7c40&title="><br>RNN 架构</p>
<h2 id="1、TextRCNN"><a href="#1、TextRCNN" class="headerlink" title="1、TextRCNN"></a>1、TextRCNN</h2><p>TextRCNN 相关论文首次发表在 AAAI 2015 中。在 TextCNN 网络中，网络结构采用“卷积层+池化层”的形式，卷积层用于提取 n-gram 类型的特征，在 RCNN（循环卷积神经网络）中，卷积层的特征提取的功能被 RNN 替代，即通过 RNN 取代 TextCNN的特征提取。RNN 的优点是能够更好地捕捉上下文信息，有利于捕获长文本的语义。因此整体结构变为了 RNN+池化层，所以叫 RCNN。</p>
<p>TextRCNN 在词嵌入的基础上加上了上下文环境作为新的词嵌入表示。左侧和右侧的context 是通过前向和后向两层 RNN 的中间层输出得到的。这些中间层的输出和原始的词嵌入拼接形成新的词嵌入 y，然后送入池化层。下图是 TextRCNN 模型框架，输入是一个文本 D，可以看成是由一系列单词（W_1, W_2,…）组成的。输出是一个概率分布，最大的位置对应文章属于的类别 K。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665385799336-b81e76eb-bfb0-469c-a9f7-56f9d812cc66.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=uf2937d25&margin=%5Bobject%20Object%5D&originHeight=442&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ucba3e876-e3a6-4a7d-8229-82ff8bafe9e&title="><br>图7. 递归卷积神经网络的结构。该图是 “A sunset stroll along the South Bank affords an array of stunning vantage points “这句话的部分例子，下标表示原句中相应的词的位置<br>RCNN 整体的模型构建流程如下：1）利用前向和后向 RNN 得到每个词的前向和后向上下文的表示，词的表示就变成词向量和前向后向上下文向量 concat 起来的形式了。2）将拼接后的向量非线性映射到低维。3）向量中的每个位置的值都取所有时序上的最大值，得到最终的特征向量。4）softmax 分类得到最终的评分向量。使用随机梯度下降来对参数进行更新。</p>
<table>
<thead>
<tr>
<th>TextRCNN</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/a5a82cbe-98b7-4f3d-87ae-f9fd59caa55e">https://sota.jiqizhixin.com/models/models/a5a82cbe-98b7-4f3d-87ae-f9fd59caa55e</a></th>
</tr>
</thead>
</table>
<h3 id="2、RNN-Capsule"><a href="#2、RNN-Capsule" class="headerlink" title="2、RNN-Capsule"></a>2、RNN-Capsule</h3><p>RNN-Capsule 是胶囊方法在文本分类中的应用，相关论文发表在 EMNLP 2018 中。胶囊网络（Capsule Network）用神经元向量代替传统神经网络的单个神经元节点，以 Dynamic Routing 的方式去训练这种全新的神经网络，从而提升模型效率及文本表达能力。</p>
<p>该模型首先利用标准的卷积网络，通过多个卷积滤波器提取句子的局部语义表征。然后将 CNN 的标量输出替换为向量输出胶囊，从而构建 Primary Capsule 层。接着输入到作者提出的改进的动态路由（共享机制的动态路由和非共享机制的动态路由），得到卷积胶囊层。最后将卷积胶囊层的胶囊压平，送入到全连接胶囊层，每个胶囊表示属于每个类别的概率。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665386285988-19f40398-a239-40ee-9d1f-051485919f34.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=uac833f15&margin=%5Bobject%20Object%5D&originHeight=419&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ubf32552e-97f7-4c47-af5b-b2d95123d69&title=">图10. 用于文本分类的胶囊网络的结构。动态路由的过程显示在底部</p>
<p>在路由过程中，许多胶囊属于背景胶囊，即这些胶囊与最终的类别胶囊无关，比如文本里的停用词、类别无关词等等。作者提出了三种策略以减少背景或者噪音胶囊对网络的影响：</p>
<ol>
<li>Orphan 类别：在胶囊网络的最后一层引入 Orphan 类别，它可以捕捉一些背景知识，比如停用词。在文本任务中停用词比较一致，比如谓词和代词等，所以引入Orphan 类别的效果较好。</li>
<li>Leaky-Softmax：在中间的连续卷积层引入去噪机制。对比 Orphan 类别，Leaky-Softmax 是一种轻量的去噪方法，它不需要额外的参数和计算量。</li>
<li>路由参数修正：传统的路由参数，通常用均与分布进行初始化，忽略了下层胶囊的概率。相反，作者把下层胶囊的概率当成路由参数的先验，改进路由过程。</li>
</ol>
<p>为了提升文本性能，作者引入了两种网络结构，具体如下：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665386285957-0aa2eef8-325a-44b7-aedb-90735ecb0e5f.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ud0795155&margin=%5Bobject%20Object%5D&originHeight=991&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u0aace264-d684-47ff-b7e8-8544d4733c6&title="></p>
<p>图11. 两种胶囊网络架构</p>
<p>Capsule-A 从嵌入层开始，将语料库中的每个词转化为 300 维（V &#x3D; 300）的词向量，然后是一个具有 32 个滤波器（B &#x3D; 32）、步长为 1 的 ReLU 非线性的 3-gram（K1 &#x3D; 3）卷积层。所有其他层都是胶囊层，从具有 32 个滤波器（C&#x3D;32）的 B×d 初级胶囊层开始，然后是具有 16 个滤波器（D&#x3D;16）的 3×C×d×d（K2&#x3D;3）卷积胶囊层和一个全连接的胶囊层，依次进行。每个胶囊都有 16 维（d&#x3D;16）的实例化参数，其长度（规范）可以描述胶囊存在的概率。胶囊层由转换矩阵连接，每个连接也要乘以路由系数，该系数由路由协议机制动态计算得出。</p>
<p>Capsule-B 的基本结构与 Capsule-A 相似，只是在 N-gram 卷积层中采用了三个平行网络，过滤窗口（N）为 3、4、5（见图 11）。全连接的胶囊层的最终输出被送入平均池以产生最终结果。通过这种方式，Capsule-B 可以学习到更有意义和更全面的文本表述。</p>
<table>
<thead>
<tr>
<th>RNN-Capsule</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/f8cd1ed1-5ebe-42bf-8672-a1d2d9c1c97f">https://sota.jiqizhixin.com/models/models/f8cd1ed1-5ebe-42bf-8672-a1d2d9c1c97f</a></th>
</tr>
</thead>
</table>
<h1 id="二、CNN"><a href="#二、CNN" class="headerlink" title="二、CNN"></a>二、CNN</h1><p>卷积神经网络（CNN）最初用于图像分类，其卷积滤波器可以提取图片的特征。与RNN不同的是，CNN可以同时将不同内核定义的卷积应用于一个序列的多个块中。对于文本分类，文本需要被表示为一个类似于图像表示的向量，并且文本特征可以从多个角度进行过滤，如图1所示。首先，输入文本的词向量被拼接成一个矩阵。然后，该矩阵被送入卷积层，该层包含不同维度的多个过滤器。最后，卷积层的结果通过集合层，并将集合结果连接起来，得到文本的最终向量表示。类别由最终的向量来预测。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665364905575-01d93bf0-e656-49ac-aa9d-8321e367cc7a.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ud052d593&margin=%5Bobject%20Object%5D&originHeight=783&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u75e85d7b-04c4-47db-9fcc-554c3c59c94&title="></p>
<h2 id="1、Text-CNN"><a href="#1、Text-CNN" class="headerlink" title="1、Text CNN"></a>1、Text CNN</h2><p>TextCNN是最经典的应用于NLP的CNN，论文发表在EMNLP 2014中。它可以通过一层卷积更好地确定最大集合层中的判别性短语，并通过保持词向量的静态来学习除词向量以外的超参数。</p>
<p>TextCNN就是简单CNN的应用，因此结构简单，效果还很好。输入数据首先通过一个嵌入层，得到输入语句的嵌入表示，然后通过一个卷积层提取语句的特征，最后通过一个全连接层得到最终的输出。模型架构如图2。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665365011892-d45b5c59-a651-4c64-9254-9b7fe1df0b1c.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=uc2f36568&margin=%5Bobject%20Object%5D&originHeight=439&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u08a0a2f9-e292-45c5-9d1c-3029e6bc046&title="><br>1）嵌入层。主要作用是将输入的自然语言编码成分布式表征，方法类似于word2vec。可以使用预训练好的词向量，也可以直接在训练TextCNN的过程中训练出一套词向量。如果使用预训练好的词向量，又分为static方法和no-static方法，前者是指在训练TextCNN过程中不再调节词向量的参数，后者在训练过程中调节词向量的参数。<br>2）卷积层。主要是通过卷积，提取不同的n-gram特征。输入的语句或者文本，通过嵌入层后转变成一个二维矩阵。假设文本的长度为|T|，词向量的大小为|d|，则该二维矩阵的大小为|T|X|d|。卷积工作就是对这一个|T|X|d|的二维矩阵进行的。<br>3）最大池化层。对卷积后得到的若干一维向量取最大值，然后拼接起来作为本层的输出值。如果卷积核的size&#x3D;2，3，4，5，每个size有128个kernel，则经过卷积层后会得到4X128个一维的向量，再经过最大池化之后，会得到4X128个scalar值，拼接在一块，得到最终的结构512X1的向量。最大池化层的意义在于对卷积提取的n-gram特征，提取激活程度最大的特征。<br>4）全连接层。在最大池化层后再拼接一层，作为输出结果。实际中为了提高网络的学习能力，可以拼接多个全连接层。</p>
<p>| ### TextCNN</p>
<p> | <a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/implements/4f2dcdb3-9422-46e6-883e-c2af64f01081">https://sota.jiqizhixin.com/implements/4f2dcdb3-9422-46e6-883e-c2af64f01081</a> |<br>| — | — |</p>
<h2 id="2、DCNN"><a href="#2、DCNN" class="headerlink" title="2、DCNN"></a>2、DCNN</h2><p>DCNN（Dynamic Convolutional Neural Network）是在ACL 2014中提出的，主要用于对句子进行语义建模，以为后续的分类或生成任务奠定基础。模型采用动态的K—max pooling（取出得分top k的特征值）处理不同长度的句子，不依赖于解析树并且适用于任何语言，利用宽卷积和k-max pooling采样，构造了一种类似parse tree的结构，能够提取长距离的信息。DCNN的架构如下图。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665366396180-087d3ec0-9820-470f-bbcc-e8068d1034c8.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u79104718&margin=%5Bobject%20Object%5D&originHeight=1172&originWidth=930&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ue5a8ac14-fa42-41f9-bd13-b7531bd0f2f&title="><br>图3 包含七个词的输入句子的DCNN。词嵌入的大小为d&#x3D;4。该网络有两个卷积层，每个层有两个特征图。这两个层的过滤器的宽度分别为3和2，k-max集合层的k值为5和3<br>1）嵌入。这一步骤和一般的网络没有区别，将输入句子s中的每个词w映射为d维向量，同时在训练的过程中对初始化的向量进行修改。</p>
<p>2）宽卷积。卷积部分用的是one-dim卷积，也就是filter的height固定为1。此外，宽卷积指的是在卷积操作时对输入矩阵的边缘进行padding补零，这样的好处是不会丢失边缘信息，其输出使feature map 的宽度更宽。</p>
<p>3）动态 k-Max Pooling。k-max pooling是max-pooling更一般的形式，相比后者，k-max在序列p中，p&gt;&#x3D;k，提取出序列中前k个最大的值，同时保留它们的相对顺序。k-max pooling的好处在于提取了句子中不止一个的重要信息，保留了它们的相对位置。并且由于最后的卷积层只需提取K个值，所以允许不同长度的输入，只要大于K。动态k-max pooling的意义在于，从不同长度的句子中提取出相应数量的语义特征信息，以保证后续的卷积层的统一性。比如，在情感预测中，首要特征所对应的单词提取出K1个，次要特征提取出k2个单词。</p>
<p>4）非线性特征函数。在k-max pooling之后，与传统的CNN一样，对于pooling后的结果加上一个偏置b进行非线性激活。</p>
<p>5）多个feature map。与传统CNN一样，会提取出多个featured map以保证提取特征的多样性。</p>
<p>6）折叠操作。之前的宽卷积是在输入矩阵d×s中的每一行内进行计算操作，其中d是word vector的维数，s是输入句子的词语数量。而折叠操作则是考虑相邻的两行之间的某种联系，将两行的vector相加；该操作没有增加参数数量，但是提前（在最后的全连接层之前）考虑了特征矩阵中行与行之间的某种关联，从而在没有增加任何参数的情况下，能够在全连接层之前提前考虑到词向量维度上的某些关联。</p>
<h2 id="3、XML-CNN"><a href="#3、XML-CNN" class="headerlink" title="3、XML-CNN"></a>3、XML-CNN</h2><p>XML-CNN发表在SIGIR 2017。这篇论文主要解决的是extreme multi-label(XML)问题，即，标签种类特别多，达到成百、上千的级别。在这种extreme multi-label情况下，容易出现数据稀疏问题，即某个label可能只有一条数据。此外，由于标签量级高，往往导致训练和预测计算量大。为了解决上述问题，作者提出利用CNN来解决XML问题。XML-CNN的架构如下图所示，从现在的角度来看，这一架构是比较简单的。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665383227053-ddccbdc3-ec96-4606-9d83-61a7cad5a58e.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u14ec5252&margin=%5Bobject%20Object%5D&originHeight=481&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ua46b97cd-d811-4d20-999b-7e4452de102&title="><br>图4 XML-CNN与样本例句。XML-CNN整体包括：嵌入层（Embedding Layer）、卷积层（Convolutional Layer）、池化层（Pooling Layer）、全连接层（FC）、分类层（sigmoid output）<br>1）嵌入层。嵌入层的输入为一个文本，文本由m个word组成，词向量维度为k，可以通过word2vec、fasttext等工具预训练得到词向量，也可以随机初始化生成。嵌入层的输出为文本向量。</p>
<p>2）卷积层。应用窗口大小为h（h-gram）进行卷积操作，主要目的是提取label对应的词特征。</p>
<p>3）池化层。池化层是应用NLP的必选步骤。一般主要应用Max-pooling以保留最显著的特征，过滤掉其它不重要的特征，同时降低模型复杂度。XML-CNN使用的是Dynamic K-Max-Pooling，与DCNN相同。</p>
<p>4）全连接层。XML-CNN在池化和输出层之间增加了一个全连接层，在论文中称为hidden bottleneck layer，即全连接层的隐藏单元数量远小于前后两个层的隐藏单元数量。XML-CNN中引入全连接层的原因一是池化层的单元太多，导致计算量太大；二是隐藏单元太多，不利于后面的表达和预测。</p>
<p>5）输出层。最终的输出层即为分类层。XML-CNN采用的是sigmoid函数，损失函数为binary entropy loss。</p>
<table>
<thead>
<tr>
<th>XML-CNN</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/06a8ba14-a96e-404b-b47a-549b606c9b3d">https://sota.jiqizhixin.com/models/models/06a8ba14-a96e-404b-b47a-549b606c9b3d</a></th>
</tr>
</thead>
</table>
<h2 id="4、TextCapsule"><a href="#4、TextCapsule" class="headerlink" title="4、TextCapsule"></a>4、TextCapsule</h2><p>TextCapsule发表在EMNLP 2018，探索将动态路由的胶囊网络与CNN相结合（区别于RNN-Capsule）。TextCapsule的架构如下图所示，主要包括四层：n-gram的卷积层、初始胶囊层、卷积胶囊层和全连接的胶囊层。另外，提出两个胶囊网络连接这4部分。<br> <img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665383385275-40bbdf95-bde0-43d1-b8f9-287945f06d0a.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ue236bd7c&margin=%5Bobject%20Object%5D&originHeight=440&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u3cfb78ac-81e0-406f-b7a6-53602277f4c&title="></p>
<p> 图5 TextCapsule架构图</p>
<p> 1）N-gram卷积层。该层是标准的卷积层，它通过各种卷积过滤器提取句子不同位置的n-gram特征。输入x为句子的表示，其中，x_i为对应于句子中第i个单词的V维单词向量。输出为B个特征图，B为过滤器数量。</p>
<p>2）初始胶囊层。这是第一层胶囊层，其中，胶囊用矢量输出胶囊替换CNN的标量输出特征检测器，以保留实例化的参数，例如单词的局部顺序和单词的语义表示。</p>
<p>3）动态路由。动态路由的基本思想是以迭代方式构造一个非线性映射，以确保每个胶囊的输出被发送到后续层中的适当父级。对于每个潜在的父对象，胶囊网络都可以通过动态路由来增加或降低各个神经元的连接强度，这比DNN中原始路由策略（Max-pooling）更为有效，该策略基本上可以检测文本的任何位置是否存在特征， 但会丢失有关要素的空间信息。作者探索了三种策略，可通过减轻某些噪声胶囊的干扰来提高路由过程的准确性：（i）孤立类别：向网络中添加了一个额外的“孤立”类别，目的是捕获文本的“背景”信息，例如停用词和与特定类别无关的词，从而帮助胶囊网络更有效率地为孩子父母关系建模。（ii）Leaky-Softmax。引入Leaky-Softmax以取代标准softmax，同时更新子胶囊与其父母之间的连接强度。尽管在最后一个胶囊层中引入了孤立类别，我们还需要在两个连续的层之间使用轻量级方法来将噪声子胶囊路由到额外的维度，而无需任何其他参数和计算量。（iii）系数修正。使用下层子胶囊中实体存在的概率来迭代地修改连接强度。</p>
<p>4）卷积胶囊层。在这一层中，每个胶囊仅连接到下层空间上的局部区域K_2 × C 。该区域中的胶囊会乘以转换矩阵以学习父子之间的关系，然后通过协议路由以在上一层中生成父胶囊。</p>
<p>5）全连接胶囊层。将下层中的胶囊平铺为一个胶囊列表，并带入到全连接胶囊层中，将胶囊乘以变换矩阵后进行协议路由，从而为每个类别生成最终胶囊和概率。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665384231748-825eca74-1e43-40cd-91f5-11e7fceb6bb1.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u905d324e&margin=%5Bobject%20Object%5D&originHeight=995&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u1c4cc776-2a2f-4c33-a4dd-ae1bb9fb535&title="><br>图6 两种胶囊网络架构<br>与RNN-Capsule类似，TextCapsule也提出了两种胶囊网络架构，如图6所示。Capsule-A从嵌入层开始，将语料库中的每个词转化为300维（V &#x3D; 300）的词向量，然后是一个具有32个滤波器（B &#x3D; 32）、步长为1的ReLU非线性的3-gram（K1 &#x3D; 3）卷积层。所有其他层都是胶囊层，从具有32个滤波器（C&#x3D;32）的B×d初级胶囊层开始，然后是具有16个滤波器（D&#x3D;16）的3×C×d×d（K2&#x3D;3）卷积胶囊层和一个全连接的胶囊层，依次进行。每个胶囊都有16维（d&#x3D;16）的实例化参数，其长度（规范）可以描述胶囊存在的概率。胶囊层由转换矩阵连接，每个连接也要乘以路由系数，该系数由路由协议机制动态计算得出。</p>
<p>Capsule-B的基本结构与Capsule-A相似，只是在N-gram卷积层中采用了三个平行网络，过滤窗口（N）为3、4、5。全连接的胶囊层的最终输出被送入平均池以产生最终结果。通过这种方式，Capsule-B可以学习到更有意义和全面的文本表示。</p>
<table>
<thead>
<tr>
<th>TextCapsule</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/3a9c6a86-16be-4d75-9a65-353bdcdffc18">https://sota.jiqizhixin.com/models/models/3a9c6a86-16be-4d75-9a65-353bdcdffc18</a></th>
</tr>
</thead>
</table>
<table>
<thead>
<tr>
<th>Bao et al.</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/d7a53e59-7faa-4258-904c-d1fd8a716b88">https://sota.jiqizhixin.com/models/models/d7a53e59-7faa-4258-904c-d1fd8a716b88</a></th>
</tr>
</thead>
</table>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>CNN和RNN在文本分类任务中表现出色。然而，这些模型不够直观，可解释性差，特别是在分类错误中，由于隐藏数据的不可读性而无法解释。随后，基于注意力的方法被成功地应用于文本分类中。基于注意力的模型架构如图8。注意力机制让模型对特定的输入给予不同的注意。它首先将重要的词聚集成句子向量，然后将重要的句子向量聚集成文本向量。它可以了解每个词和句子对分类判断的贡献有多大，这有利于通过两个层次的关注进行应用和分析。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665384524690-c117b49c-2054-41b6-a9ca-5ff00414d8bd.png#clientId=u92da87b6-5fd9-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=udfaf4c2e&margin=%5Bobject%20Object%5D&originHeight=771&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u29a11c7c-6a75-4691-a071-c7e9dfd1423&title="><br>图8 注意力机制架构</p>
<h2 id="1、AttentionXML"><a href="#1、AttentionXML" class="headerlink" title="1、AttentionXML"></a>1、AttentionXML</h2><p><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/839883d4-569c-4d5c-9457-e5a374375875">https://sota.jiqizhixin.com/models/models/839883d4-569c-4d5c-9457-e5a374375875</a></p>
<h1 id="三、Transformer"><a href="#三、Transformer" class="headerlink" title="三、Transformer"></a>三、Transformer</h1><table>
<thead>
<tr>
<th>模型</th>
<th>SOTA！模型资源站收录情况</th>
<th>模型来源论文</th>
</tr>
</thead>
<tbody><tr>
<td>ELMo</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/e473edac-88cc-4a83-b533-d15c2ca0ea17">https://sota.jiqizhixin.com/models/models/e473edac-88cc-4a83-b533-d15c2ca0ea17</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：3</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow、PyTorch、MXNet 等</td>
<td>Deep Contextualized Word Representations</td>
<td></td>
</tr>
<tr>
<td>GPT</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/09877886-65c3-44b0-a5be-9e5043964787">https://sota.jiqizhixin.com/models/models/09877886-65c3-44b0-a5be-9e5043964787</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow、PyTorch</td>
<td>Improving language understanding by generative pre-training</td>
<td></td>
</tr>
<tr>
<td>BERT</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/1d27a74a-8668-4d64-9615-ce889b88700b">https://sota.jiqizhixin.com/models/models/1d27a74a-8668-4d64-9615-ce889b88700b</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：7</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow、PyTorch 等</td>
<td>BERT: pre-training of deep bidirectional transformers for language understanding</td>
<td></td>
</tr>
<tr>
<td>ALBERT</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/0306236e-ce20-4886-98cb-45730fda6cf9">https://sota.jiqizhixin.com/models/models/0306236e-ce20-4886-98cb-45730fda6cf9</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：13</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：PyTorch、TensorFlow、CANN</td>
<td>ALBERT: A lite BERT for self-supervised learning of language representations</td>
<td></td>
</tr>
<tr>
<td>X-Transformer</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/fa67640c-fedb-400e-a6fb-e83b4eedbb65">https://sota.jiqizhixin.com/models/models/fa67640c-fedb-400e-a6fb-e83b4eedbb65</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow</td>
<td>Taming pretrained transformers for extreme multi-label text classification</td>
<td></td>
</tr>
<tr>
<td>LightXML</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/7f477837-72fe-435d-b8e4-13d5308c7bfa">https://sota.jiqizhixin.com/models/models/7f477837-72fe-435d-b8e4-13d5308c7bfa</a></td>
<td></td>
</tr>
<tr>
<td>支持框架：PyTorch</td>
<td>Lightxml: Transformer with dynamic negative sampling</td>
<td></td>
</tr>
<tr>
<td>for high-performance extreme multi-label text classification</td>
<td></td>
<td></td>
</tr>
<tr>
<td>TextGCN</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/64a6edf7-3e03-455c-bb54-26ab72607c0e">https://sota.jiqizhixin.com/models/models/64a6edf7-3e03-455c-bb54-26ab72607c0e</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow</td>
<td>Graph convolutional networks for text classification</td>
<td></td>
</tr>
<tr>
<td>TensorGCN</td>
<td><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/2702592d-f69f-41a6-9c29-924999576bd3">https://sota.jiqizhixin.com/models/models/2702592d-f69f-41a6-9c29-924999576bd3</a></td>
<td></td>
</tr>
<tr>
<td>收录实现数量：1</td>
<td></td>
<td></td>
</tr>
<tr>
<td>支持框架：TensorFlow</td>
<td>Tensor graph convolutional networks for text classification</td>
<td></td>
</tr>
</tbody></table>
<h2 id="1、Pre-train"><a href="#1、Pre-train" class="headerlink" title="1、Pre-train"></a>1、Pre-train</h2><p>预训练的语言模型可以有效地学习全局语义代表，并显著提升NLP任务的效果，包括文本分类。它通常使用无监督的方法来自动挖掘语义知识，然后构建预训练目标，使机器能够学习理解语义。如图1所示，我们给出了Embeddingfrom Language Model (ELMo)、OpenAI GPT和BERT之间模型架构的差异。ELMo是一个深度的语境化单词表示模型，它很容易被整合到模型中。它可以对单词的复杂特征进行建模，并为各种语言环境学习不同的表示方法。它根据双向LSTM的上下文词来学习每个词的嵌入。GPT采用有监督的微调和无监督的预训练来学习一般的表征，这些表征在有限的适应下转移到许多NLP任务。此外，目标数据集的领域不需要与未标记的数据集的领域相似。GPT算法的训练过程通常包括两个阶段。首先，神经网络模型的初始参数是通过在未标记的数据集上的建模目标学习的。我们可以采用相应的监督目标来适应目标任务的这些参数。谷歌提出的BERT模型，通过对每一层的左右上下文进行联合调节，从未标记的文本中预训练出深层次的双向表征，显著提高了NLP任务的性能，包括文本分类。BERT应用了双向编码器，旨在通过联合调整所有层的上下文来预先训练深度的双向表示。它可以在预测哪些词被掩盖时利用上下文信息。它只需增加一个额外的输出层就可以进行微调，为多个NLP任务构建模型。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665400887880-9a2dfc91-7138-4ec8-9719-79678f794822.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ub5213396&margin=%5Bobject%20Object%5D&originHeight=242&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u053343b2-35df-4b30-b150-32a3c27a11b&title="><br>与这三个模型相比，ELMo是一种使用LSTM的基于特征的方法，而BERT和OpenAI GPT是使用Transformer的微调方法（也可以归类到本文第七类Transformer模型里，为了表述清晰，本文将GPT和BERT放入Pre-train类别中介绍）。此外，ELMo和BERT是双向训练模型，而OpenAI GPT是从左到右的训练。因此，BERT得到了一个更好的结果，它结合了ELMo和OpenAI GPT的优点。</p>
<h2 id="2、ELMo"><a href="#2、ELMo" class="headerlink" title="2、ELMo"></a>2、ELMo</h2><p>词向量模型ELMo的相关论文发表在NAACL 2018上。具体的方法是将每个词的表示看作是对于整体句子的函数，通过利用在语料上训练的双向LSTM的语言模型得到词向量，因此将其称为ELMo(Embeddings from Language Models)。对比传统Word2Vec这种静态形式的词向量，ELMo是一种动态模型。对于静态形式的词向量来说，无论在任何的上下文中都使用同一个向量，因此很难表示一词多义的现象，而ELMo则可以通过上下文动态生成词向量，从理论上会是更好的模型，从实测效果来看在很多任务上也都达到了当时的SOTA成绩。ELMo引入了上下文语境的概念。ELMo的架构如下图所示。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401462267-016982c7-8f31-4c42-ac89-ce60fad8930c.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u2c099ec6&margin=%5Bobject%20Object%5D&originHeight=542&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u2521f996-2839-4dbc-b685-ed512e5edce&title=">图2 ELMo架构<br>首先介绍双向语言模型biLM，这是ELMo的基础。给定N个tokens的序列，一个前向语言模型通过对t_o的概率进行建模来计算t_k的概率：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401601612-3427754b-5493-49b2-b8e0-328c2d1d4ae8.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=88&id=u90af6f74&margin=%5Bobject%20Object%5D&originHeight=156&originWidth=893&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ue54b1626-937a-4753-a187-2718710531d&title=&width=501"><br>而双向语言模型就是添加了另一个方向的预测，利用N~k+1个token来预测第k个token：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401616286-df9ac0f3-a680-4410-88cc-7a0f591ceed3.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=82&id=uae3c14a2&margin=%5Bobject%20Object%5D&originHeight=149&originWidth=967&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u3cfa2aca-ae71-4709-9d54-d198a8c765f&title=&width=532"><br>目标函数则是上述两者的联合损失（BiLM）：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401630236-797671f0-b67c-4c78-a006-821729c28db0.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=104&id=u9bb95a09&margin=%5Bobject%20Object%5D&originHeight=256&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ued22b1b2-8689-42c8-ad83-5a6b4999394&title=&width=440"><br>不同于其他类型的模型采用最顶层的输出作为token的表示，ElMo采用多个层的线性加和作为token的表示，对于每个token，在一个L层的biLm中可以输出2L+1表示：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401645857-4d13dcb8-20cb-41d5-b976-e3555debedca.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=92&id=uc04468d1&margin=%5Bobject%20Object%5D&originHeight=193&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u27556eb6-4acf-47c4-96ec-7421029e100&title=&width=513"><br>在下游任务中，可以将上述2L+1个表示进行整合：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665401664132-985b6d61-56ea-467e-bb6c-7ca81b7e8332.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=78&id=u5afded3d&margin=%5Bobject%20Object%5D&originHeight=164&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=udce531b3-e921-4c0e-a347-03e50245b22&title=&width=512"><br>具体到ELMo的架构，还是按照训练语言模型的方式，使用了CNN-BIG-LSTM结构和一个层之间的残差链接。使用ELMo可以针对一个token产生三个向量,：原始向量、第一层向量、第二层向量。作者认为低层的bi-LSTM层能提取语料中的句法信息，而高层的bi-LSTM能提取语料中的语义信息。</p>
<p>| ### ELMo</p>
<p> | <a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/e473edac-88cc-4a83-b533-d15c2ca0ea17">https://sota.jiqizhixin.com/models/models/e473edac-88cc-4a83-b533-d15c2ca0ea17</a> |<br>| — | — |</p>
<h2 id="3、GPT"><a href="#3、GPT" class="headerlink" title="3、GPT"></a>3、GPT</h2><p>GPT是“Generative Pre-Training”的简称，是指的生成式的预训练。GPT的训练程序包括两个阶段。第一阶段的预训练是在一个大型文本语料库上学习一个高容量的语言模型。接下来是一个微调阶段，在这个阶段，使模型适应带有标记数据的判别性任务。</p>
<ul>
<li>第一阶段的工作具体为：Embedding——&gt;Transformer——&gt;Text Production。</li>
<li>第二阶段的工作具体为：将下游任务的网络结构改造成和GPT的网络结构是一样的；利用第一步预训练好的参数初始化GPT的网络结构完成下游任务，这样通过预训练学到的语言学知识就被引入到任务里了；使用下游任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决任务的问题。</li>
</ul>
<p>对于特定任务的输入转换，使用了一种遍历式的方法，将结构化的输入转换成预先训练好的模型可以处理的有序序列。图3提供了一个可视化的说明。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665402048867-897707af-b7a7-49f9-b3d5-3db1103c5f8f.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u5b6e0c12&margin=%5Bobject%20Object%5D&originHeight=436&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u30ae7708-a976-4d94-a346-645ceed2ec2&title="><br><strong><em>图3 左）GPT中使用的Transformer结构和训练目标。(右) 输入在不同的任务上进行微调的转化。将所有结构化的输入转化为标记性的序列，由预训练模型处理，然后是一个线性+softmax层</em></strong></p>
<table>
<thead>
<tr>
<th>GPT</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/09877886-65c3-44b0-a5be-9e5043964787">https://sota.jiqizhixin.com/models/models/09877886-65c3-44b0-a5be-9e5043964787</a></th>
</tr>
</thead>
</table>
<h2 id="4、BERT"><a href="#4、BERT" class="headerlink" title="4、BERT"></a>4、BERT</h2><p>BERT（Bidirectional Encoder Representations from Transformers）自从谷歌提出就一直大热，很多人认为BERT是整个NLP研究历程中里程碑似的节点。BERT与ELMo提出，一经提出就明显胜过了ELMo。BERT的灵感来源是OpenAI在17年发布过的一篇名为“Attention is all your need”论文中提到的Transformer模型。关于Transformer的相关模型我们放在第七个章节中具体介绍，本章节内BERT是作为一类预训练模型介绍的。</p>
<p>首先，BERT在NLP的11个任务（包括文本分类任务）中均有较大程度性能提升。其次，BERT在无监督场景下结合预训练能够最大化地利用文本数据，同时对不同场景的下游任务均有提升和帮助。BERT的基础建立在Transformer之上，拥有强大的语言表征能力和特征提取能力，同时再次证明了双向语言模型的能力更加强大。<br>首先，BERT在NLP的11个任务（包括文本分类任务）中均有较大程度性能提升。其次，BERT在无监督场景下结合预训练能够最大化地利用文本数据，同时对不同场景的下游任务均有提升和帮助。BERT的基础建立在Transformer之上，拥有强大的语言表征能力和特征提取能力，同时再次证明了双向语言模型的能力更加强大。</p>
<p>BERT的网络结构使用了双向Transformer的堆叠，Encoder和Decoder分别12层。其思想出于ELMo和GPT但同时又高于二者。ELMo采用了双向LSTM来训练词Embedding，虽然使用了双向LSTM，但其实是使用2个单向LSTM对学到的句子语义信息做拼接，和BERT完全双向不同，对句子间不同词的语义依赖关系也不如BERT捕捉的充分。GPT只有单向，其假设句子间语义依赖关系只有从左到右，而没有从右到左，该假设在实际中并不完全满足。BERT加入了Masked Language Model(MLM) 和 Next Sentences Prediction(NSP），使得模型能够在无监督的场景下学习到句子间特征和语义特征。在无监督学习场景训练，能最大化的使用训练语料。而Pre-train和Fine-tune能够方便地将已训练好的BERT模型迁移到不同的应用场景，在工业界大有益处。</p>
<p>图1最右侧为BERT的架构，可以看出BERT的最大亮点：<strong>双向Transformer网络，BERT直接引用了Transformer架构中的Encoder模块，并舍弃了Decoder模块, 这样便自动拥有了双向编码能力和强大的特征提取能力。</strong></p>
<p>下图为BERT的输入图示。由图示可知，BERT的输入包括三个部分：词嵌入张量（Token embeddings）、语句分块张量（ segmentation embeddings）、位置编码张量（position embeddings），最终的embedding向量是将上述的3个向量直接做加和的结果。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665402393369-3a6bc91e-adc4-41ee-9810-a083fa830c17.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=ua32c6e61&margin=%5Bobject%20Object%5D&originHeight=332&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u5359fe12-d402-403b-ac74-c0042ac768f&title="><br>图4 BERT输入表示。输入嵌入是标记嵌入、分割嵌入和位置嵌入的总和。嵌入和位置嵌入的总和<br>接下来，<strong>MLM和 NSP是BERT的另外两个亮点</strong>：<br><strong>BERT中引入了一个带mask的语言模型训练（Masked LM）</strong>。在原始训练文本中，随机抽取15%的token作为即将参与mask的对象。在选中的token中，数据生成器并不是把他们全部变成[MASK]，具体变化方法包括三类：一是，在80%的概率下，用[MASK]标记替换token，比如my dog is hairy → my dog is [MASK]；二是，在10%的概率下，用随机单词替换token，比如my dog is hairy → my dog is apple；三是，在10%的概率下，保持token不变，比如my dog is hairy → my dog is hairy。</p>
<p><strong>BERT还引入了一个下句话预测任务（Next Sentence Prediction ）</strong>，目的是服务问答、推理、句主题关系等NLP任务。所有的参与任务训练的语句都被选中参加，其中：50%的B是原始本中实际跟随A的下句话；50%的B是原始本中随机抽取的一句话。在该任务中，BERT模型可以在测试集上取得97-98%的准确率。<br>最后，<strong>fine-tuning是BERT的另一个亮点</strong>，只需要将特定任务的输入，输出插入到BERT中，利用Transformer强大的注意力机制就可以模拟很多下游任务，从而具有极佳的迁移特性。</p>
<table>
<thead>
<tr>
<th>BERT</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/1d27a74a-8668-4d64-9615-ce889b88700b">https://sota.jiqizhixin.com/models/models/1d27a74a-8668-4d64-9615-ce889b88700b</a></th>
</tr>
</thead>
</table>
<h2 id="5、ALBERT"><a href="#5、ALBERT" class="headerlink" title="5、ALBERT"></a>5、ALBERT</h2><p>ALBERT（A Lite BERT）被称为Lite版的Bert，相关论文发表在ICLR 2020。ALBERT采用了两种技术来减少参数量，以应对扩展预训练模型的主要障碍。</p>
<ul>
<li><strong>第一个是因子化嵌入参数化。</strong>通过将大型词汇嵌入矩阵分解成两个小矩阵，将隐藏层的大小与词汇嵌入的大小分开。这种分离使得在不显著增加词汇嵌入的参数大小的情况下更容易增长隐藏的大小。</li>
<li><strong>第二种技术是跨层参数共享。</strong>这种技术可以防止参数随着网络的深度而增长。这两种技术都大大减少了BERT的参数数量而不严重损害性能，从而提高了参数效率。类似于BERT的ALBERT配置的参数减少了18倍，训练速度可以提高1.7倍左右。减少参数的技术也作为一种正则化的形式，稳定了训练并有助于泛化。为了进一步提高ALBERT的性能，还引入了一个用于句序预测的自监督损失（sentence-order prediction，SOP）。SOP主要关注句子间的连贯性，旨在解决原始BERT中提出的下句预测（NSP）损失的无效性。</li>
</ul>
<p><strong>ALBERT架构的主干与BERT相似，因为它使用具有GELU非线性的Transformer encoder。将词汇embedding size表示为E，将encoder layers层数表示为L，将hidden size表示为H，将feed forward&#x2F;filter size设置为4H，多头注意力的数量设置为H &#x2F; 64。</strong></p>
<p>让BERT模型变大（scale up）的方法一般有两种：变深（增加模型的层数）、变宽 （增加隐藏尺寸，即每一层embeddings的特征数），ALBERT的方式是第二种。具体ALBERT的改进点如下所示：</p>
<ul>
<li>Factorized Embedding Parameterization（将初始嵌入缩短为128个特征）。对于词向量维度 E 和隐层维度 H，在词表 V 到 H 的中间，插入一个小维度 E，多做一次尺度变换：O(VxH) →  O(VxE+ExH)。</li>
<li>Cross-layer Parameter Sharing（共享所有层的参数）。具体分为三种模式：只共享 attention 相关参数、只共享 FFN 相关参数、共享所有参数（attention相关+FFN）。全共享如下图所示：</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665402762274-6f7ca780-c02f-48cb-95e6-bc9c3be0561e.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u7311ef9a&margin=%5Bobject%20Object%5D&originHeight=300&originWidth=300&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u4a5fac66-85a5-41c3-b641-766e2b159ff&title="><br> 图5  ALBERT全共享参数</p>
<ul>
<li>Sentence Order Prediction（SOP）。SOP 就是让模型预测两个相邻 segments 有没有被调换前后顺序。SOP主要是针对NSP提出的，即句子顺序判断。NSP将话题预测和连贯性预测混合在一个任务中。然而，与连贯性预测相比，话题预测更容易学习，而且与使用MLM[掩蔽语言建模]损失学习的内容重叠更多。因此，ALBERT提出一个主要基于连贯性的损失，即句序预测（SOP）损失，它避免了话题预测，侧重于对句子间的一致性进行建模。</li>
<li>此外，还包括用长句做预训练、随机遮连续的多个词（Masked-ngram-LM vs Masked LM）、扩大模型参数、增加预训练数据、去掉Dropout层等trick。<table>
<thead>
<tr>
<th>ALBERT</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/0306236e-ce20-4886-98cb-45730fda6cf9">https://sota.jiqizhixin.com/models/models/0306236e-ce20-4886-98cb-45730fda6cf9</a></th>
</tr>
</thead>
</table>
</li>
</ul>
<h1 id="四、Transfomer"><a href="#四、Transfomer" class="headerlink" title="四、Transfomer"></a>四、Transfomer</h1><p>基于Transformer的模型可以在不考虑顺序信息的情况下将计算并行化，适用于大规模的数据集，使其在NLP任务中很受欢迎。Transformer由17年一篇著名论文“Attention is All Your Need”提出的。在这篇论文中，作者提出了一种全新的注意力机制— self-Attention，做到了仅仅利用Attention代替了传统的RNN，实现了快速并行计算，挖掘了DNN的特性。</p>
<p>Transformer主要分为两个部分：编码组件+解码组件，与传统的RNN编码、解码的结构很像。Transformer的编码组件由多个encoder(编码器)堆叠而成。每个encoder又有两个子层，分别是 FFN(前馈神经网络) + self-Attention(自注意力层)。FFN就是MLP（多层感知器架构）。而self-Attention层的输入是一个单词经过词嵌入处理的句子，也就是一个词向量列表，输出是结合了句子本身上下文注意力之后的融合向量。此外，为了能够从不同角度捕捉不同的关联程度，Transformer利用了一种”多头”的self-Attention机制。即一个self-Attention层拥有多组WQ 、WK 、WV ，每组分别用于不同的特征提取，把所有“头”通过一个高维矩阵W0 。Transformer的架构如下图：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403035375-f757a82e-8a4c-49ec-badf-64dba3076ede.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u00bf8b08&margin=%5Bobject%20Object%5D&originHeight=685&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uf6614e57-d875-48c9-8af8-3f6e21a970d&title="><br>图 6 Transfomer架构图<br>我们在之前介绍过，BERT系列模型其实也是Transformer模型，但归类为预训练的方法进行了介绍。本章节则聚焦于专门的Transformer系列模型。</p>
<h2 id="1、X-Transformer"><a href="#1、X-Transformer" class="headerlink" title="1、X-Transformer"></a>1、X-Transformer</h2><p>X-Transformer的工作发表在KDD 2020中，主要针对的是极端多标签（Extreme Multi-label  XML）文本分类问题，即给定输入文本，从大型标签集中返回最相关的标签。因此，X-Transformer也是聚焦于稀疏的巨大标签空间问题（XML-CNN、AttentionXML等），只不过方法是deep transformer结构。X-Transformer的架构如下图：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403135446-35ae7ad1-c747-4285-bd65-4b24b62348f5.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u7fcedebc&margin=%5Bobject%20Object%5D&originHeight=314&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ud84cfab8-9a4b-49c3-b88d-bf408420b14&title="></p>
<p><strong><em>图7 X-Transformer框架。首先，语义标签索引减少了庞大的输出空间。然后在XMC子问题上对Transformer进行微调，该子问题将实例映射到标签集群。最后，在集群和转化器的输出上有条件地训练线性排名器，以便在预测的集群中对标签进行重新排名</em></strong><br>1）语义标签索引（Semantic Label Indexing，SLI）。利用层次聚类将XMC问题转化为更小的K个输出空间的子问题。给定一个训练集D，XML的目的是学习一个评分函数f，将输入（或实例）x_i和 一个标签l映射到一个分数f(x_i, l) ∈R。优化函数f，以满足当y_il&#x3D;1时得分高（即，标签l与实例x_i有关），而当y_il&#x3D;0时，得分较低：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403178934-82e41d18-daf8-4597-9c94-7e6e836ac5c6.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=83&id=u60609a70&margin=%5Bobject%20Object%5D&originHeight=180&originWidth=842&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u99e2a40d-f260-4b21-ab81-4499dc6f963&title=&width=388"><br>其中，ϕ(x)代表一个编码，W 是分类器的瓶颈层。ϕ(x)的编码采用两种方式：一是，通过标签文本嵌入标签，利用XLNET的对每一个token进行词嵌入(token_dim&#x3D;1 × 1024 )后求和然后平均池化得到标签的向量表示；二是，通过正面实例的嵌入进行标签嵌入。<br>2）作为神经匹配器的deep Transformer。经过SLI，原来难以解决的XML问题演变成了一个可行的XML子问题，其输出空间大小为K，是一个非常小的数。应用deep Transformer将每个文本实例映射到指定的相关集群。构建分类器：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403201804-1862fedd-a347-4d97-975d-9f22fad3d140.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=55&id=ub54f1737&margin=%5Bobject%20Object%5D&originHeight=104&originWidth=1028&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u9dc548ea-2e0e-4184-aac4-bbf0e92ff0b&title=&width=544"><br>训练时的Loss为squared hinge loss：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403216449-30dbbf5b-de03-4fa9-b093-fce5362070f7.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=100&id=uac3bc3b7&margin=%5Bobject%20Object%5D&originHeight=267&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u270dfc77-a618-4f49-90f8-f3b0b84cb1f&title=&width=405"><br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403226870-2b43b877-7599-415a-ba9d-d98593568877.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=35&id=u07bc8466&margin=%5Bobject%20Object%5D&originHeight=58&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u9a113350-e763-4169-881d-6fe4bf8d109&title=&width=658"><br>3）排序。用一个线性排序器对所属簇类标签进行排序。引入两个tricks：一是，Teacher Forcing Negatives (TFN)。为了在反向更新权重时提升运算效率并节省运算空间，在back propagation时一般都要进行负采样。本文在负采样抽样时只包含了与groudtruth l 标签属于同一簇的实例。二是，Matcher-aware Negatives (MAN)。为了解决exposure bias的问题，选择使用了MAM方法。该方法借鉴了Scheduled Sampling的思路，将groudtruth与上一个隐状态预测的label所在的簇相互融合。</p>
<p>| ### X-Transformer</p>
<p> | <a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/fa67640c-fedb-400e-a6fb-e83b4eedbb65">https://sota.jiqizhixin.com/models/models/fa67640c-fedb-400e-a6fb-e83b4eedbb65</a> |<br>| — | — |</p>
<h2 id="2-Light-XML"><a href="#2-Light-XML" class="headerlink" title="2. Light - XML"></a>2. Light - XML</h2><p>Light-XML是AAAI2021中最新的关于Transformer的模型，也是针对于XML问题提出的。在我们前面的介绍中，<strong>AttentionXML和X-Transformer存在两个共同的问题：一是，模型既大又多。对于一个数据集，AttentionXML需要训练四个模型。X-Transformer也需要分阶段训练两个模型。二是，负采样策略都关注于一些困难样本，或者说与正样本相似的样本，此种采样方法较难使得模型收敛。</strong>LightXML的思路是：结合Transformer和Generative Cooperative Networks将分阶段模型变成End-to-End模型，同时使用动态负采样策略让模型更容易收敛，从而获得更好的效果。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403381649-3282b9c5-2610-49b6-a637-89184747047d.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u4ec5f452&margin=%5Bobject%20Object%5D&originHeight=628&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=ua0c98dc0-3d7c-4776-8923-6d61f2abd1a&title="><br>图 8  Light-XML 架构图<br>1）标签聚类。聚类方法与AttentionXML相同。由于两层的标签树以及足够处理XML问题，因此只设置了两层的标签树。<br>2）文本表征。与X-Transformer相同，采用了三种预训练Transformer模型，BERT、XLNet、RoBERTa。区别在于Light-XML对于小型的XML数据集采用了512的输入长度，以及concatenate最后五层的[CLS]表示，作为模型输出。<br>3)标签回溯。生成器：经过一个sigmoid激活的全连接网络(原文中称为Generator)，生成K个标签簇的分数。选择其中top-b个标签簇的所有标签作为候选标签。在训练阶段会为候选标签加入所有positive的标签。使用交叉熵作为Generator的损失。动态负标签采样（Dynamically NLS）：X-Transformer&#x2F;AttentionXML是在第一阶段模型训练完成后，将其结果作为负采样的依据。而动态负采样方法对于同一个训练样本，其负标签在每一个epoch下重新采样，从而能够保证负样本由易到难，同时防止过拟合。<br>4）排序。标签嵌入：通过随机初始化的标签嵌入矩阵获取候选标签的嵌入；Hidden Bottleneck Layer：参考上文我们介绍的XML-CNN的设置，在分类层和表示层之间加入一个低维的全连接层，起到降低参数量和增强非线性的作用。Discriminator：压缩原始文档表示，与标签嵌入相乘后得到最终所有候选标签的分数。最终使用交叉熵损失。</p>
<table>
<thead>
<tr>
<th>Light - XML</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/7f477837-72fe-435d-b8e4-13d5308c7bfa">https://sota.jiqizhixin.com/models/models/7f477837-72fe-435d-b8e4-13d5308c7bfa</a></th>
</tr>
</thead>
</table>
<h1 id="五、GNN"><a href="#五、GNN" class="headerlink" title="五、GNN"></a>五、GNN</h1><p>随着图神经网络（GNN）的关注度越来越高，基于GNN的模型通过对句子的句法结构进行编码，在语义角色标签任务、关系分类任务和机器翻译任务中获得了出色的表现。它将文本分类变成了一个图节点分类任务。我们展示了一个用于文本分类的GCN模型，有四个输入文本，如下图所示。首先，将四个输入文本𝑇&#x3D;[𝑇1,𝑇2,𝑇3,𝑇4]和文本中的词𝑋&#x3D;[𝑥1, 𝑥2, 𝑥3, 𝑥4, 𝑥5, 𝑥6]定义为节点，构造成图结构。图形节点由黑色粗边连接，这表示文档-词边和词-词边。每个词-词边缘的权重通常意味着它们在语料库中的共同出现频率。然后，单词和文本通过隐藏层表示。最后，所有输入文本的标签可以通过图来预测。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403660108-78302138-113f-4848-b3ca-f572d16b5bfd.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=uc8fe62df&margin=%5Bobject%20Object%5D&originHeight=367&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u9c15241a-f96c-42db-934b-2c7cff1a811&title="><br><strong><em>图9 基于GNN的模型。初始图的不同取决于图的设计方式。给出一个例子，在文档与文档、文档与句子、词与词之间建立边</em></strong></p>
<h2 id="1、TextGCN"><a href="#1、TextGCN" class="headerlink" title="1、TextGCN"></a>1、TextGCN</h2><p>TextGCN的文章发表在AAAI 2019中。TextGCN从整个语料库中构建了一个大图，其中包含了作为节点的词和文件，使用图卷积网络（Graph Convolutional Network，GCN）对该图进行建模，这是一个简单而有效的图形神经网络，可以捕捉到高阶邻域信息。两个词节点之间的边缘是由词的共现信息建立的，而一个词节点和文档节点之间的边缘是用词频和词的文档频率建立的。然后，<strong>将文本分类问题转化为节点分类问题。</strong>该方法可以在标注文档比例较小的情况下实现强大的分类性能，并学习到可解释的词和文档节点嵌入。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665403809206-1878387f-2e8d-40dc-8347-3c651b0dad50.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u5082a859&margin=%5Bobject%20Object%5D&originHeight=421&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u1cdc3e3a-2b0f-45cc-b016-b739afcbb2c&title="><br><strong><em>图10 文本GCN的示意图。样本取自Ohsumed语料库。以 “O “开头的节点是文档节点，其他节点是词节点。黑色粗边是文档-词的边，灰色细边是词-词的边。R(x)表示x的表示（嵌入）。不同的颜色表示不同的文档类别。CVD：心血管疾病，Neo: 肿瘤，Resp: 呼吸道疾病，Immun: 免疫学疾病</em></strong><br>具体到TextGCN，其图卷积结构如下：<br>1）节点特征向量。把特征矩阵X &#x3D; I设置为一个单位矩阵，每个单词或文档都表示为one-hot向量，作为Text GCN的输入。</p>
<p>2）边权重。TextGCN根据单词在文档中的出现率（文档-单词边）和单词在整个语料库中的共现率（单词-单词边）来建立节点之间的边。一个文档节点和一个单词节点之间的边的权重是该单词在文档中的术语频率-逆文档频率（term frequency-inverse document frequency，TF-IDF），其中术语频率是该单词在文档中出现的次数，逆文档频率是包含该单词的文档数量的对数比例的反分数。具体来说，对语料库中的所有文档使用固定大小的滑动窗口来收集共现统计信息，采用点对点的相互信息（point-wise mutual information，PMI）来计算两个词语节点之间的权重。形式上，节点i和节点j之间的边的权重被定义为：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404150275-fe0eba1d-6e61-45a4-9182-ea557fdfa717.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=118&id=u59dbde5e&margin=%5Bobject%20Object%5D&originHeight=249&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uda482937-0a03-4dcb-800c-0f5aa711681&title=&width=513"><br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404207217-2dea8387-5e35-47e8-a1a4-3b726c553c81.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=229&id=u7fa462ed&margin=%5Bobject%20Object%5D&originHeight=552&originWidth=688&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uf792413c-555e-4607-b618-4b43b3002a8&title=&width=285"><br>在建立文本图后，将图送入一个简单的两层GCN，第二层节点（词&#x2F;文档）嵌入的大小与标签集相同，并送入一个softmax分类器，最后再经过交叉熵做分类。两层的GCN可以允许最多两步远的节点之间的信息传递。因此，尽管图中没有直接的文档-文档边，但两层GCN允许文档对之间的信息交互。</p>
<table>
<thead>
<tr>
<th>TextGCN</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/64a6edf7-3e03-455c-bb54-26ab72607c0e">https://sota.jiqizhixin.com/models/models/64a6edf7-3e03-455c-bb54-26ab72607c0e</a></th>
</tr>
</thead>
</table>
<h2 id="2、TensorGCN"><a href="#2、TensorGCN" class="headerlink" title="2、TensorGCN"></a>2、TensorGCN</h2><p>TensorGCN相关论文发表在AAAI 2020中。TensorGCN构造一个文本图张量（<strong>多个图</strong>）来描述语义、句法和上下文信息。然后，对文本图张量进行了两种传播学习。第一种是图内传播，用于在单个图中聚合来自邻近节点的信息。第二种是图间传播，用于协调图之间的异构信息。TensorGCN提供了一种有效的方法来协调和集成来自不同类型图的异构信息，提升了文本分类的效果。TensorGCN的架构如下图所示：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404398978-da65d6e2-db88-4aa6-a590-a94feba6e69b.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u5ce64b6c&margin=%5Bobject%20Object%5D&originHeight=298&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uecb1bb71-0e00-4802-bcca-f12b5733289&title="><br><strong><em>图11 应用于文本分类的TensorGCN架构</em></strong><br>与TextGCN不同，TensorGCN应用的是张量图，张量中每个图的结点都是相同的，而结点之间的边以及权重不同。张量图表示为：G &#x3D;(G1;G2; ···;Gr)，其中Gi &#x3D; (Vi;Ei;Ai)，Vi表示第i个图张量的结点集合，对于从同一段文本派生出来的图张量，Vi&#x3D;Vj，Ai≠Aj。邻接矩阵也被定义成张量的形式：A &#x3D; (A1;A2; ··· ;Ar) ，其维度为r×n×n。<br>我们需要在节点之间建立两种边：词-文本边和词-字边。词-文本边是根据文档中出现的词建立的，边的权重用TF-IDF方法计算。根据三种不同的语言属性建立词-字边：语义信息、句法依赖和局部顺序语境。基于这些不同种类的词-字边，构建了一系列的文本图来描述文本文件。<br>1）Semantic-based graph。<br>（1）对指定任务的训练数据进行LSTM训练。<br>（2）使用LSTM获取语料库中每个文本&#x2F;句子中的所有单词的语义特征&#x2F;嵌入。<br>（3）根据语料库上的词的语义嵌入计算word-word的边权重。对于每个句子&#x2F;文本，从训练后的LSTM输出中获得单词的语义特征&#x2F;嵌入，并计算单词之间的余弦相似度。当相似度超过阈值，就认为两个单词之间具有语义关系。然后根据单词之间语义关系出现的频率计算结点之间的边权值。</p>
<p>2）Syntactic-based graph。使用Stanford CoreNLP解析器来提取单词之间的依赖关系。虽然提取的依赖项是定向的，但为了简单起见，将其视为无定向关系。与Semantic-based graph中使用的策略类似，计算了在整个语料库中每对具有句法依赖的单词的次数，并计算了每对单词的边权重(基于语法的图中的节点)，如图12所示。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404464738-6c47023b-98b2-4d8a-bc32-f444add73fc5.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u7eb37cc5&margin=%5Bobject%20Object%5D&originHeight=186&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u7627155e-cf76-4cb9-b033-bfdc18ef5e7&title="><br><strong><em>图12 利用LSTM编码的语义信息建立单词之间的关系。以一份文本为例，基于语义的图是通过收集所有文本语料库中的所有语义关系词对来构建的</em></strong></p>
<p>3）Sequential-based graph。序列语境描述了局部共现（词与词之间）的语言属性，已被广泛用于文本表示学习。在这项研究中，利用滑动窗口策略和PMI来描述序列语境信息。</p>
<p>对于graph tensor，所有的图共享相同的节点集，唯一的区别是边。为了将上述三种不同的图聚合在一起，通过池化操作进行权值选取。不过，由于不同权重代表的含义不同，所以不能直接进行简单的池化。引入注意力机制，将分别乘上不同注意力的权值加在一起，代替池化操作：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404529054-6b7314df-996d-45a7-b255-9dd68d1f059d.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=35&id=uf283a3ee&margin=%5Bobject%20Object%5D&originHeight=64&originWidth=946&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u49f9c114-c9c9-42e9-a462-49d192ddc35&title=&width=517"><br>其中，W是需要学习的注意力矩阵，与邻接矩阵A的size相同。TensorGCN的图内传播和图间传播学习过程如图13：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404555876-429c3774-b6ff-467f-bc22-cb15abbea76b.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&id=u4a5b49aa&margin=%5Bobject%20Object%5D&originHeight=285&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uc8c91245-a4eb-4f26-9fe0-20afcd23103&title="><br><strong><em>图13 以具有三个单词节点和三个文档节点的文本图张量为例，展示TensorGCN学习过程中的一个层。(a)图内传播从输入图张量学习；（b）图间传播学习，使用图内传播的输出作为其输入。这里仅以一个虚拟图为例，说明如何通过图间传播学习来协调异质信息。在实践中，所有的虚拟图都必须进行图间传播学习</em></strong><br>图内传播就是在三种不同的图的内部分别进行GCN的卷积操作：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404623366-db22445a-7d56-4fff-a514-75666f5a4d61.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=41&id=u9e6a632c&margin=%5Bobject%20Object%5D&originHeight=68&originWidth=770&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uf8ed1386-9d50-4e69-b595-bb222802669&title=&width=469"><br> 图间传播学习在不同的graph之间交换信息（图13 b），为此定义了一系列特殊的图，称为virtual graph，通过连接张量中所有图的节点来实现，每一个结点都产生一个r*r大小的虚图，最终得到了图间邻接矩阵的tensor。每个虚结点之间都有边，并且权重设置为1。虚拟图上的图间信息传播学习定义如下：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/1143997/1665404648284-daef4e88-62c1-4a9c-9911-8856c4009448.png#clientId=u5870d980-5fcc-4&crop=0&crop=0&crop=1&crop=1&from=paste&height=49&id=ueee598da&margin=%5Bobject%20Object%5D&originHeight=74&originWidth=860&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uc0e6f8f8-a3ed-4ab7-9d35-5f1f5240302&title=&width=572"><br>在TensorGCN的最后一层，在完成图间传播之后，对图执行一个平均池，以获得用于分类的文档节点的最终表示。</p>
<table>
<thead>
<tr>
<th>TensorGCN</th>
<th><a target="_blank" rel="noopener" href="https://sota.jiqizhixin.com/models/models/2702592d-f69f-41a6-9c29-924999576bd3">https://sota.jiqizhixin.com/models/models/2702592d-f69f-41a6-9c29-924999576bd3</a></th>
</tr>
</thead>
</table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">阿泽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://rongzhiy.github.io/2022/10/10/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8BSOTA/">https://rongzhiy.github.io/2022/10/10/文本分类模型SOTA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://rongzhiy.github.io" target="_blank">Rongzhiyのblog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/11/%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%91%E5%8E%9F%E7%94%9F%EF%BC%9F/"><img class="prev-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">什么是云原生？</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">设计模式</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">阿泽</div><div class="author-info__description">Stay Hungry,Stay Foolish</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">204</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">43</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/rongzhiy"><i class="fab fa-github"></i><span>github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/rongzhiy" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:rongzhiy2735@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81-RNN"><span class="toc-text">一、 RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81TextRCNN"><span class="toc-text">1、TextRCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81RNN-Capsule"><span class="toc-text">2、RNN-Capsule</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81CNN"><span class="toc-text">二、CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Text-CNN"><span class="toc-text">1、Text CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81DCNN"><span class="toc-text">2、DCNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81XML-CNN"><span class="toc-text">3、XML-CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81TextCapsule"><span class="toc-text">4、TextCapsule</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention"><span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81AttentionXML"><span class="toc-text">1、AttentionXML</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81Transformer"><span class="toc-text">三、Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81Pre-train"><span class="toc-text">1、Pre-train</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81ELMo"><span class="toc-text">2、ELMo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81GPT"><span class="toc-text">3、GPT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81BERT"><span class="toc-text">4、BERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81ALBERT"><span class="toc-text">5、ALBERT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Transfomer"><span class="toc-text">四、Transfomer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81X-Transformer"><span class="toc-text">1、X-Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Light-XML"><span class="toc-text">2. Light - XML</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81GNN"><span class="toc-text">五、GNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81TextGCN"><span class="toc-text">1、TextGCN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81TensorGCN"><span class="toc-text">2、TensorGCN</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/tools/maven/maven-core-concepts/" title="Maven 核心概念总结"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Maven 核心概念总结"/></a><div class="content"><a class="title" href="/2023/03/17/tools/maven/maven-core-concepts/" title="Maven 核心概念总结">Maven 核心概念总结</a><time datetime="2023-03-17T09:00:32.696Z" title="发表于 2023-03-17 17:00:32">2023-03-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/tools/gradle/gradle-core-concepts/" title="Gradle 核心概念总结"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Gradle 核心概念总结"/></a><div class="content"><a class="title" href="/2023/03/17/tools/gradle/gradle-core-concepts/" title="Gradle 核心概念总结">Gradle 核心概念总结</a><time datetime="2023-03-17T09:00:32.691Z" title="发表于 2023-03-17 17:00:32">2023-03-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/tools/git/github-tips/" title="Github 实用小技巧总结"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Github 实用小技巧总结"/></a><div class="content"><a class="title" href="/2023/03/17/tools/git/github-tips/" title="Github 实用小技巧总结">Github 实用小技巧总结</a><time datetime="2023-03-17T09:00:32.687Z" title="发表于 2023-03-17 17:00:32">2023-03-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/tools/git/git-intro/" title="Git 核心概念总结"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 核心概念总结"/></a><div class="content"><a class="title" href="/2023/03/17/tools/git/git-intro/" title="Git 核心概念总结">Git 核心概念总结</a><time datetime="2023-03-17T09:00:32.685Z" title="发表于 2023-03-17 17:00:32">2023-03-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/tools/docker/docker-intro/" title="Docker 核心概念总结"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker 核心概念总结"/></a><div class="content"><a class="title" href="/2023/03/17/tools/docker/docker-intro/" title="Docker 核心概念总结">Docker 核心概念总结</a><time datetime="2023-03-17T09:00:32.681Z" title="发表于 2023-03-17 17:00:32">2023-03-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 阿泽</div><div class="framework-info"><span>框架 </span><a href="">Hexo</a></div></div><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/cat.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>